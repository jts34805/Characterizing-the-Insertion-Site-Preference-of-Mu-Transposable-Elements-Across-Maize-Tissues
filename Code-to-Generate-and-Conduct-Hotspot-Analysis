#This file is for describing the code ran to generate MuCounts.rda, Locs.rda, and Hotspots.rda, as well as analysis associated with these R objects. 
#Functions required are available in file "R-Functions"
#create constants
EffectiveGenomeSize=2182075994 #calculated using genmap, this is the effective genome size for B73 V5

##load in packages
library(ComplexHeatmap)


##load in R objects
load("HotSpotsCleanData.rda") #this object was generated from file "Data-processing-1-Read-Filtering-and-Molecule-Counting"

###Generate MuCounts object from HotSpotsCleanData object
#Process data by requiring 5 reads per UMI, remove molecules with "N" in their UMI, and run MergeFn 
CleanData2 = lapply(HotSpotsCleanData, function(xx) { xx[xx$Reads >= mean(xx$Reads)/5,] })
CleanData2 = lapply(CleanData2, function(xx) { xx[!grepl('N', xx$UMI),] }) #removes any reads thats UMI includes an "N"
MoleculeCounts = MergeFn(CleanData2)  # Merge CleanData list into a matrix of Mu TIR counts
rm(CleanData2, CleanData)

MuCounts2 = MuCountFn(MoleculeCounts)  # Connect both TIRs and convert to matrix of Mu Insertion Counts
MuCounts3 = MuStagger(MuCounts2)  # Connect TIRs with shift from 9 bp TSD... this primarily affects inherited counts for small number of paternal insertions
nrow(MuCounts3[[1]]) - nrow(MuCounts2[[1]])  # 104 insertion sites were affected by the 'MuStagger' function (out of ~3.8 million)
MuCounts = adjCounts(MuCounts3, thresh = 1, pseudo = 500)  # For Mu insertions where there was a large discrepancy between the left and right borders, estimate MuCounts based on only the better-sampled border. This affects 258 insertions (0.00013% of total)
rm(MoleculeCounts, MuCounts2, MuCounts3)

keeps = rowSums(MuCounts[[1]]) > 0
MuCounts = lapply(MuCounts, function(xx) { xx[keeps,] })
MuCounts=poolTechReps(MuCounts)
MuCounts=MuCounts[[1]] #removes the second and third object from the MuCounts list, not needed for the analysis

#filter inherited insertions from MuCounts
MuCountsCPM=sweep(MuCounts,2,colSums(MuCounts),"/")*10^6 #normalize by library depth to better identify the inherited insertions from the dataset
plant = gsub('[a-z]', '', colnames(MuCounts), ignore.case=T)
plant=gsub(".+-","",plant)
plant=gsub("_","",plant)
samps = unique(plant[grepl('E', colnames(MuCounts))])
FindShared = MuCountsCPM[,plant %in% samps]
plant = plant[plant %in% samps]
paternals=list()
historicals = rowMeans(MuCountsCPM[,grepl('MI', colnames(MuCountsCPM))] >= 1000) == 1
for (s in samps) {
  X2 = FindShared[,plant == s] >= 1000
  X2=as.data.frame(X2)
  if (ncol(X2) == 2) {
    paternals[[s]] = (rowMeans(X2) == 1) & !historicals}
  
  if (ncol(X2)>2) {
    paternals[[s]] = X2[,grepl('E', colnames(X2))] & (rowSums(X2[,!grepl('E', colnames(X2))]) >= 1) & !historicals
    
  }
}
AllInherited=gsub(".+c","c",names(which(unlist(paternals))>0)) #takes the list made from the above loop and identifies the positions of all inherited insertions across our dataset. 
MuCounts=MuCounts[!rownames(MuCounts)%in%AllInherited,] #removed 916 unique sites from the matrix that were inherited (>1000 CPM insertions in endo and matched tissue)
historicals=historicals[historicals>0] #find the true statements for historical insertion sites, as these are the histroical insertion sites
MuCounts=MuCounts[!rownames(MuCounts)%in%names(historicals),] #remove the rows from MuCounts that are historical, inherited insertions 
save(MuCounts, file = 'MuCounts.rda')

##Make Locs object from MuCounts object
Locs = t(matrix(unlist(strsplit(rownames(MuCounts),':')), nrow = 2))
Locs1=strsplit(Locs[,2],",")
Locs1=lapply(Locs1, function(X){round(mean(as.numeric(X)))}) #taking the average of LB RB 
Locs = data.frame(chr = Locs[,1], position = unlist(Locs1))
Locs$bin = paste(Locs$chr, round(Locs$pos/2000), sep = ',')
save(Locs, file="Locs.rda")

#make HotSpots from Locs object, trim and process
HotSpots = FindHotSpots(Locs)  # Identify Mu hotspots using overall insertion locations
HotSpots$size = apply(HotSpots[,2:3], 1, diff) + 1 #add size of hotspot column to dataframe

##add tissue information to initial called hotspots
HScounts = matrix2bincounts(MuCounts) #count the number of insertions detected from each sample for each hotspot
zygotics = matrix2bincounts(MuCounts >= 500)  # Identifies which hotspots have insertions contributed to by early, abundant Mu insertions
zygotics = which(rowSums(zygotics[-1,]) > 0) #identifies hotspots that are contributed to by early Mu insertions, which add unwanted signal for samples from the same individual
HScpm = sweep(HScounts[-1,], 2, colSums(HScounts), '/')*10^6  # Scale hotspot counts to insertions per million
HScpmScaled = log(HScpm[-zygotics,colSums(HScounts[-1,]) >= 10000] + 10)  #Remove hotspots that contain any zygotic insertions (they are dominated by lineage signal) require at least 10K insertions in a sample, then log transform after adding a pseudocount of 10
save(HScpmScaled, file="HScpmScaled.rda")


HSCPMScaledandFiltered = HScpmScaled[rowSums(HScpm[-zygotics,colSums(HScounts[-1,]) >= 10000] >= 50) >= 3,] #keeps only hotspots that arent zygotic and that have >=50 cpm in 3 samples that have >=10000 cpm 

#group samples by tissue type, run F test to identify hotspots that are have significant differences in the number of insertions contributed to each hotspot by each tissue
tissue=gsub('.+_',"",colnames(D))
tissueMeans = matrix(NA, nrow = nrow(D2), ncol = length(unique(tissue)))
colnames(tissueMeans) = unique(tissue)

for (z in unique(tissue)) { tissueMeans[,z] = rowMeans(D2[,tissue == z]) }

var_exp = rowSums(sweep(sweep(tissueMeans, 1, rowMeans(D2), '-')^2, 2, table(tissue)[colnames(tissueMeans)], '*'))/(ncol(tissueMeans) - 1)
var_unexp = rowSums((D2 - tissueMeans[,tissue])^2)/(ncol(D2) - ncol(tissueMeans))
Ft = var_exp/var_unexp
p.Ft = pf(Ft, ncol(tissueMeans) - 1, ncol(D2) - ncol(tissueMeans), lower.tail = F)

sum(p.adjust(p.Ft,method='BH') <= .01)  # 1558 significantly Tissue associated hotspots
###end of F test##

HotSpots$pval_unadj = p.Ft[rownames(HotSpots)] # add tissue specific p value to each respective hotspot
HotSpots$FDRts = p.adjust(p.Ft, method = 'BH')[rownames(HotSpots)] #calculate tissue specific FDR for each significant hotspot
mtchs = match(rownames(HotSpots),rownames(D))
HotSpots$Endosperm_FoldEnrichment = rowMeans(exp(D[mtchs,tissue == 'E']) - 10) #add fold enrichment measurement for each tissue to each hotspot
HotSpots$Leaf_FoldEnrichment = rowMeans(exp(D[mtchs,tissue == 'L']) - 10)
HotSpots$Pollen_FoldEnrichment = rowMeans(exp(D[mtchs,tissue == 'P']) - 10)
HotSpots$Root_FoldEnrichment = rowMeans(exp(D[mtchs,tissue == 'R']) - 10)
HotSpots[,8:11] = round(sweep(HotSpots[,8:11], 1, HotSpots[,5], '/')*2182075994/10^6,2) 

#trim hotspots 100 bp in size
HotSpots2 = HotSpots2[HotSpots2$size >= 100,]

HotSpots2$Score = apply(HotSpots2[,4:5], 1, score)

#this next section was ran on a computing cluster because of the large memory and core needs to generate 1000 null distributions
library(parallel)
set.seed(1)
load("Locs.rda")
n_cores =28
Locs_base = Locs[, 1:2]
n_iter = 1000
HSnull = vector("list", n_iter)

HSnull = mclapply(
  X = seq_len(n_iter),
  FUN = function(i) {
    cat("NULL", i, "\n")

    LocsNULL = Locs_base
    LocsNULL$position = Locs_base$position +
      round((runif(nrow(Locs_base)) - 0.5) * 1e6)

    FindHotSpots(LocsNULL) #FindHotSpots function can be found in "R-Functions" file in this repository
    },
  mc.cores = n_cores
)
###End of what was run on the computing cluster###

HSnull= HSnull[sapply(HSnull, is.data.frame)]
HSnull = lapply(HSnull, function(X) {
  X$size = abs(apply(X[, 2:3], 1, diff)) + 1
  X
}) #get hotspot sizes for null distributions
HSnull = lapply(HSnull, function(X){X$score=apply(X[,4:5], 1, score)}) #calculate "score" for each null hotspot using hotspot size and insertions detected in the hotspot
HSnullScores = unlist(HSnull)

FDRs = rank(-c(HotSpots2$Score, HSnullScores), ties.method = 'max')[1:nrow(HotSpots2)] #rank all actual and null called hotspots for FDR correction
FDRs2 = rank(-HotSpots2$Score, ties.method = 'max')
HotSpots2$FDR = cummax((FDRs - FDRs2)/FDRs2/1000) #calculate FDR
HotSpots2 = HotSpots2[order(-HotSpots2$Score),] #order hotspots with most significant listed first
HotSpots2=HotSpots2[HotSpots2$FDR<=0.01,] #Apply FDR cutoff of <= 0.01, resulting in 14022 called hotspots
save(HotSpots2, file="HotSpots2.rda")








